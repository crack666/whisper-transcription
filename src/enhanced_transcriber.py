"""
ADAPTIVE MODE IMPROVEMENTS - 2025-05-28
========================================

The adaptive mode has been enhanced to integrate defensive silence principles
and eliminate duplicate transcriptions caused by overlapping segments.

Key Improvements:
1. ‚úÖ Three-tier strategy: defensive silence ‚Üí enhanced detection ‚Üí defensive-guided fixed-time
2. ‚úÖ Non-overlapping segments prevent duplicate transcriptions
3. ‚úÖ Maintains adaptive intelligence for different speaker patterns
4. ‚úÖ 100% segment success rate with clean boundaries

Performance Results:
- Improved Adaptive: 344 words, 4 segments, clean output (no overlaps)
- Defensive Silence: 352 words, 4 segments, 7x faster processing
- Fixed Time 30s: 378 words, 6 segments, but with overlapping duplicates

The adaptive mode is now the default and provides the best balance of
accuracy and natural segmentation without duplicates.
"""

import whisper
import torch
import time
import logging
import numpy as np
from pydub import AudioSegment
from typing import List, Tuple, Dict, Optional, Any
from pathlib import Path
from tqdm import tqdm
from concurrent.futures import ProcessPoolExecutor, as_completed
import multiprocessing as mp

from .waveform_analyzer import WaveformAnalyzer # Assuming precision_waveform_detection uses this
from .segmentation_handler import SegmentationHandler # New import

logger = logging.getLogger(__name__)

# Placeholder for LANGUAGE_MAP if not imported from elsewhere
LANGUAGE_MAP = {"german": "de", "english": "en"}


def _transcribe_segment_isolated_worker(work_item: Tuple[int, Tuple, str, str, str, Dict]) -> Dict[str, Any]:
    """
    Isolated worker function for parallel segment transcription.
    Each worker loads its own model to avoid CUDA/GPU conflicts.
    
    Args:
        work_item: Tuple of (index, segment_info, model_name, language, device, config)
            - index: Original position in segment list (for sorting)
            - segment_info: (segment_file, original_start, original_end, padded_start, padded_end)
            - model_name: Whisper model to load
            - language: Language code
            - device: Device for inference
            - config: Transcription config dict
    
    Returns:
        Dictionary with transcription result and metadata:
        {
            'index': int,           # Original position
            'start': float,         # Start time in seconds
            'end': float,           # End time in seconds  
            'text': str,            # Transcribed text
            'segment_file': str,    # Path to segment file
            'confidence': float,    # Optional confidence score
            'quality_score': float, # Optional quality score
            'warnings': list,       # Optional warnings
            'error': str           # Optional error message
        }
    """
    idx, segment_info, model_name, language, device, config = work_item
    segment_file, original_start, original_end, padded_start, padded_end = segment_info
    
    try:
        # Each worker loads its own model instance
        import whisper
        import torch
        
        if device is None:
            device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # Load model in this process
        model = whisper.load_model(model_name, device=device)
        
        # Transcription options
        transcribe_options = {
            "language": language,
            "verbose": False,
            "fp16": True if device == "cuda" else False,
            "condition_on_previous_text": True,
            "initial_prompt": "Dies ist eine Aufzeichnung einer deutschen Universit√§tsvorlesung.",
        }
        
        # Add enhanced options if configured
        if config.get('use_beam_search', False):
            transcribe_options.update({
                "beam_size": 5,
                "best_of": config.get('best_of', 3),
                "patience": config.get('patience', 2.0),
                "length_penalty": config.get('length_penalty', 0.8),
            })
        
        # Transcribe segment
        result = model.transcribe(segment_file, **transcribe_options)
        
        # Return with metadata
        return {
            'index': idx,
            'start': original_start / 1000.0,  # Convert to seconds
            'end': original_end / 1000.0,
            'text': result.get('text', '').strip(),
            'segment_file': segment_file,
            'confidence': None,  # Could be calculated from result if needed
            'quality_score': None,
            'warnings': None
        }
        
    except Exception as e:
        logger.error(f"Worker failed on segment {idx} ({segment_file}): {e}")
        return {
            'index': idx,
            'start': original_start / 1000.0,
            'end': original_end / 1000.0,
            'text': f"[Error transcribing segment: {e}]",
            'segment_file': segment_file,
            'error': str(e)
        }


class EnhancedAudioTranscriber:
    """
    Enhanced audio transcriber optimized for slow speakers with long pauses.
    """
    
    def __init__(self, 
                 model_name: str = "large-v3", 
                 language: str = "german", 
                 device: Optional[str] = None,
                 config: Optional[Dict] = None):
        """
        Initialize the enhanced audio transcriber.
        
        Args:
            model_name: Whisper model to use
            language: Language for transcription
            device: Device for inference (cpu, cuda, etc.)
            config: Additional configuration options
        """
        self.model_name = model_name
        self.language = LANGUAGE_MAP.get(language.lower(), "de")
        self.device = device
        self.model = None
        
        # Enhanced config for slow speakers
        self.config = self._get_enhanced_config()
        if config:
            self.config.update(config)
        
        self.segmentation_handler = SegmentationHandler(self.config) # Initialize SegmentationHandler
        self.waveform_analyzer = WaveformAnalyzer(config=self.config) # Initialize WaveformAnalyzer
        
        logger.info(f"Initialized EnhancedAudioTranscriber with segmentation_mode={self.config.get('segmentation_mode', 'defensive_silence')}, disable_segmentation={self.config.get('disable_segmentation', False)}")
    
    def _get_enhanced_config(self) -> Dict:
        """Get enhanced configuration optimized for robust transcription."""
        return {
            # Segmentation control
            'disable_segmentation': False,          # NEW: Set to True to process entire file without splitting
            
            # Segmentation mode - Options: 'fixed_time', 'defensive_silence', 'precision_waveform'
            'segmentation_mode': 'defensive_silence', 
            'fixed_time_duration': 30000,              # 30 seconds per segment when using fixed_time mode
            'fixed_time_overlap': 3000,                # 3 seconds overlap for fixed_time segments
            
            # Parameters for defensive_silence_detection and padding
            'min_silence_len': 2000,        # 2 seconds
            'padding': 1500,                # 1.5 seconds padding
            'silence_adjustment': 5.0,      # Tolerance for background noise (used by older methods, review if still needed by kept ones)
            
            # General segmentation parameters
            'max_segment_length': 240000,   # 4 minutes max per segment
            'min_segment_length': 2000,     # 2 seconds minimum
            'overlap_duration': 3000,       # 3 seconds overlap for segment creation (used by create_overlapping_segments)
            
            # Quality settings
            'enable_vad': True,             # Voice Activity Detection (general concept)
            'vad_threshold': 0.3,           # Lower threshold for quiet speech (used by older methods, review)
            'normalize_audio': True,        # Normalize volume levels
            
            # Whisper-specific optimizations
            'use_beam_search': True,
            'best_of': 3,
            'patience': 2.0,
            'length_penalty': 0.8,
            
            # Post-processing
            'merge_short_segments': True,
            'validate_transcription': True,
            'cleanup_segments': True,
            'create_waveform_visualization': False, # For precision_waveform_detection
            'segment_prefix': 'segment', # Added for consistency
            
            # Parallel processing (experimental - DEPRECATED, use batch_size instead)
            'parallel_workers': 1,  # Number of parallel workers (1=sequential, >1=parallel)
            
            # TRUE Batch Processing (NEW! - Encoder batching with single model)
            'batch_size': 3,  # Number of segments to encode in parallel (3-4 optimal for 32GB VRAM)
        }
    
    def load_model(self) -> None:
        """Load the Whisper model if not already loaded."""
        if self.model is None:
            import torch
            
            if self.device is None:
                self.device = "cuda" if torch.cuda.is_available() else "cpu"
            
            logger.info(f"Loading Whisper model: {self.model_name} on {self.device}")
            self.model = whisper.load_model(self.model_name, device=self.device)
            logger.info("Model loaded successfully")
    
    def analyze_speech_patterns(self, audio_file: str) -> Dict:
        """
        Robust speech pattern analysis that handles silent sections properly.
        
        Args:
            audio_file: Path to audio file
            
        Returns:
            Analysis results with recommended settings
        """
        logger.info("Analyzing speech patterns...")
        
        audio = AudioSegment.from_file(audio_file)
        duration = len(audio) / 1000.0  # seconds
        
        # Analyze in chunks, filtering out completely silent ones
        chunk_size = 5000  # 5 second chunks for better analysis
        volumes = []
        speech_chunks = 0
        total_chunks = 0
        
        # Process chunks with progress bar
        for i in tqdm(range(0, len(audio), chunk_size), desc="üìä Analyzing audio patterns", unit="chunks", leave=True):
            chunk = audio[i:i+chunk_size]
            if len(chunk) > 1000:  # At least 1 second
                chunk_db = chunk.dBFS
                total_chunks += 1
                
                # Only include non-silent chunks in volume analysis
                if chunk_db > -60:  # Not completely silent
                    volumes.append(chunk_db)
                    if chunk_db > -40:  # Likely speech
                        speech_chunks += 1
        
        # Calculate safe statistics
        if volumes:
            # Filter out -inf values that cause NaN
            finite_volumes = [v for v in volumes if np.isfinite(v)]
            if finite_volumes:
                mean_volume = np.mean(finite_volumes)
                volume_std = np.std(finite_volumes)
            else:
                # Fallback for all -inf volumes
                mean_volume = -50
                volume_std = 10
        else:
            # Fallback for completely silent audio
            mean_volume = -60
            volume_std = 10
        
        # Calculate ratios safely
        speech_ratio = speech_chunks / total_chunks if total_chunks > 0 else 0
        quiet_ratio = 1 - speech_ratio
        
        # More robust speaker type classification
        if speech_ratio < 0.3:
            speaker_type = "very_sparse_speech"
            recommended_silence_len = 3000
            recommended_padding = 2000
        elif speech_ratio < 0.5:
            speaker_type = "sparse_speech" 
            recommended_silence_len = 2500
            recommended_padding = 1500
        elif speech_ratio < 0.7:
            speaker_type = "moderate_speech"
            recommended_silence_len = 2000
            recommended_padding = 1000
        else:
            speaker_type = "dense_speech"
            recommended_silence_len = 1500
            recommended_padding = 750
        
        analysis = {
            "duration_seconds": duration,
            "mean_volume_db": mean_volume,
            "volume_std": volume_std,
            "quiet_ratio": quiet_ratio,
            "speech_ratio": speech_ratio,
            "speaker_type": speaker_type,
            "recommended_min_silence_len": recommended_silence_len,
            "recommended_padding": recommended_padding,
            "estimated_speech_time": speech_chunks * (chunk_size / 1000),
            "total_chunks_analyzed": total_chunks,
            "speech_chunks": speech_chunks,
            "analysis_method": "robust_enhanced"
        }
        
        # logger.info(f"Speaker type: {speaker_type}")
        # logger.info(f"Speech ratio: {speech_ratio:.2f}, Quiet ratio: {quiet_ratio:.2f}")
        
        return analysis
    
    # VAD methods are now part of WaveformAnalyzer or could be separate functions
    # For simplicity, let's assume they are part of WaveformAnalyzer or imported
    
    def defensive_silence_detection(self, audio: AudioSegment, analysis: Dict) -> List[Tuple[int, int]]:
        """
        Detects speech segments using a defensive approach to silence detection.
        This method is now a wrapper around WaveformAnalyzer's implementation.
        """
        logger.info("Using defensive_silence_detection via WaveformAnalyzer")
        return self.waveform_analyzer.defensive_silence_detection(
            audio=audio, 
            min_silence_len=self.config.get('min_silence_len', 2000),
            silence_thresh_offset=self.config.get('silence_adjustment', 5.0),
            padding_ms=self.config.get('padding', 1500),
            analysis=analysis # Pass the analysis dict
        )

    def precision_waveform_detection(self, audio: AudioSegment, analysis: Dict) -> List[Tuple[int, int]]:
        """
        Detects speech segments using precision waveform analysis.
        This method is now a wrapper around WaveformAnalyzer's implementation.
        """
        logger.info("Using precision_waveform_detection via WaveformAnalyzer")
        # The actual call to WaveformAnalyzer's method
        return self.waveform_analyzer.detect_speech_segments(audio_segment=audio)

    def transcribe_with_enhanced_options(self, segment_file: str) -> Dict[str, Any]:
        """
        Transcribe with enhanced Whisper options for better accuracy.
        
        Args:
            segment_file: Path to audio segment
            
        Returns:
            Enhanced transcription result
        """
        transcribe_options = {
            "language": self.language,
            "verbose": False,
            "fp16": True,
            "condition_on_previous_text": True,
            "initial_prompt": "Dies ist eine Aufzeichnung einer deutschen Universit√§tsvorlesung.",
        }
        
        # Add enhanced options if available in this Whisper version
        if self.config.get('use_beam_search', False):
            transcribe_options.update({
                "beam_size": 5,
                "best_of": self.config.get('best_of', 3),
                "patience": self.config.get('patience', 2.0),
                "length_penalty": self.config.get('length_penalty', 0.8),
            })
        
        try:
            result = self.model.transcribe(segment_file, **transcribe_options)
            return result
        except Exception as e:
            # Fallback to basic transcription
            logger.warning(f"Enhanced transcription failed, using basic options: {e}")
            basic_options = {
                "language": self.language,
                "verbose": False,
                "fp16": False,  # Fallback without FP16
                "condition_on_previous_text": True
            }
            return self.model.transcribe(segment_file, **basic_options)
    
    def validate_transcription_segment(self, result: Dict, segment_info: Tuple[str, int, int]) -> Dict:
        """
        Validate and enhance transcription results.
        
        Args:
            result: Whisper transcription result
            segment_info: Segment information
            
        Returns:
            Validated and enhanced result
        """
        segment_file, start_time, end_time = segment_info
        text = result.get("text", "").strip()
        
        # Quality metrics
        quality_score = 1.0
        warnings = []
        
        # Check for very short transcriptions relative to audio length
        duration_seconds = (end_time - start_time) / 1000.0
        if duration_seconds > 10 and len(text) < 20:
            quality_score *= 0.5
            warnings.append("Very short transcription for long audio segment")
        
        # Check for repetitive text (possible transcription error)
        words = text.split()
        if len(words) > 5:
            unique_words = len(set(words))
            repetition_ratio = unique_words / len(words)
            if repetition_ratio < 0.3:
                quality_score *= 0.7
                warnings.append("High repetition detected")
        
        # Check average confidence if available
        avg_confidence = result.get("confidence", 1.0)
        if avg_confidence < 0.5:
            quality_score *= 0.8
            warnings.append("Low confidence score")
        
        return {
            "text": text,
            "confidence": avg_confidence,
            "quality_score": quality_score,
            "warnings": warnings,
            "duration_seconds": duration_seconds,
            "words_per_minute": len(words) / (duration_seconds / 60) if duration_seconds > 0 else 0
        }
    
    def _verify_timeline_integrity(self, segments: List[Dict]) -> bool:
        """
        Verify that segments are in chronological order with valid timestamps.
        
        Args:
            segments: List of transcribed segments with 'start' and 'end' keys
            
        Returns:
            True if timeline is valid, False otherwise
        """
        if not segments:
            return True
        
        for i in range(len(segments) - 1):
            current_end = segments[i].get('end', 0)
            next_start = segments[i+1].get('start', 0)
            
            # Check for timeline violations (overlaps or reversed order)
            if current_end > next_start + 0.1:  # Allow 0.1s tolerance for floating point
                logger.error(
                    f"‚ùå Timeline violation detected: "
                    f"Segment {i} ends at {current_end:.2f}s, "
                    f"but segment {i+1} starts at {next_start:.2f}s"
                )
                return False
            
            # Check for negative durations
            if segments[i].get('end', 0) <= segments[i].get('start', 0):
                logger.error(f"‚ùå Invalid segment {i}: end <= start")
                return False
        
        logger.info(f"‚úÖ Timeline integrity verified: {len(segments)} segments in correct chronological order")
        return True
    
    def _batch_encode_segments(self, segment_files: List[str]) -> torch.Tensor:
        """
        üöÄ TRUE BATCH ENCODING: Encode multiple segments in parallel using ONE model!
        
        This is the key optimization: Instead of loading model N times (multi-process),
        we encode all segments together in a single forward pass.
        
        Args:
            segment_files: List of audio file paths to encode
            
        Returns:
            Batch of encoded audio features (batch_size, 1500, 384)
        """
        logger.debug(f"üéØ Batch encoding {len(segment_files)} segments...")
        
        # Get correct n_mels for this model (80 for older models, 128 for v3/turbo)
        n_mels = self.model.dims.n_mels
        
        # Load and convert all audios to mel spectrograms
        mel_list = []
        for segment_file in segment_files:
            audio = whisper.load_audio(segment_file)
            mel = whisper.log_mel_spectrogram(torch.from_numpy(audio), n_mels=n_mels)
            mel_list.append(mel)
        
        # Pad to same length (required for batching)
        max_len = max(mel.shape[-1] for mel in mel_list)
        padded_mels = []
        for mel in mel_list:
            if mel.shape[-1] < max_len:
                padding = torch.zeros(mel.shape[0], max_len - mel.shape[-1])
                mel = torch.cat([mel, padding], dim=1)
            padded_mels.append(mel)
        
        # Stack to batch (batch_size, 80, frames)
        mel_batch = torch.stack(padded_mels).to(self.model.device)
        
        # Encode ALL in ONE forward pass! üöÄ
        with torch.no_grad():
            audio_features_batch = self.model.encoder(mel_batch)
        
        logger.debug(f"   ‚úÖ Batch encoded: {audio_features_batch.shape}")
        return audio_features_batch
    
    def _batch_decode_features(self, audio_features_batch: torch.Tensor, segment_infos: List[Tuple]) -> List[Dict]:
        """
        Decode batch of audio features sequentially (autoregressive limitation).
        
        Args:
            audio_features_batch: Encoded features (batch_size, 1500, 384)
            segment_infos: List of (segment_file, original_start, original_end, ...)
            
        Returns:
            List of transcribed segments
        """
        results = []
        
        decode_options = whisper.DecodingOptions(
            language=self.language,
            fp16=torch.cuda.is_available(),
            without_timestamps=False
        )
        
        # Decode each (autoregressive - must be sequential)
        for i, (audio_features, segment_info) in enumerate(zip(audio_features_batch, segment_infos)):
            segment_file, original_start, original_end = segment_info[:3]
            
            try:
                result = whisper.decode(self.model, audio_features.unsqueeze(0), decode_options)
                text = result[0].text.strip()
                
                results.append({
                    'start': original_start / 1000.0,
                    'end': original_end / 1000.0,
                    'text': text,
                    'segment_file': segment_file,
                    'confidence': None,
                    'quality_score': None,
                    'warnings': None
                })
                
            except Exception as e:
                logger.error(f"Decode failed for segment {i}: {e}")
                results.append({
                    'start': original_start / 1000.0,
                    'end': original_end / 1000.0,
                    'text': f"[Decoding error: {e}]",
                    'segment_file': segment_file,
                    'error': str(e)
                })
        
        return results
    
    def _transcribe_segments_parallel(self, processed_segments: List[Tuple], max_workers: int = 4) -> List[Dict]:
        """
        Transcribe segments in parallel using multiple workers.
        Each worker loads its own model to avoid GPU conflicts.
        Results are sorted by original index to maintain timeline order.
        
        Args:
            processed_segments: List of (segment_file, original_start, original_end, padded_start, padded_end)
            max_workers: Number of parallel workers
            
        Returns:
            List of transcribed segments in original chronological order
        """
        logger.info(f"üöÄ Starting parallel transcription with {max_workers} workers for {len(processed_segments)} segments")
        logger.info(f"   Using 'spawn' method for CUDA compatibility")
        
        # Prepare work items with index for each segment
        work_items = [
            (idx, seg_info, self.model_name, self.language, self.device, self.config)
            for idx, seg_info in enumerate(processed_segments)
        ]
        
        results = []
        
        try:
            # Use 'spawn' method for CUDA compatibility (required for PyTorch multiprocessing)
            ctx = mp.get_context('spawn')
            with ProcessPoolExecutor(max_workers=max_workers, mp_context=ctx) as executor:
                # Submit all tasks
                future_to_index = {
                    executor.submit(_transcribe_segment_isolated_worker, work_item): work_item[0]
                    for work_item in work_items
                }
                
                # Collect results as they complete with progress bar
                for future in tqdm(
                    as_completed(future_to_index),
                    total=len(work_items),
                    desc="üé§ Transcribing segments (parallel)",
                    unit="segment"
                ):
                    idx = future_to_index[future]
                    try:
                        result = future.result()
                        results.append(result)
                    except Exception as e:
                        logger.error(f"Failed to get result for segment {idx}: {e}")
                        # Create error placeholder
                        seg_info = processed_segments[idx]
                        results.append({
                            'index': idx,
                            'start': seg_info[1] / 1000.0,
                            'end': seg_info[2] / 1000.0,
                            'text': f"[Transcription failed: {e}]",
                            'segment_file': seg_info[0],
                            'error': str(e)
                        })
        
        except Exception as e:
            logger.error(f"Parallel processing executor failed: {e}")
            raise
        
        # CRITICAL: Sort by original index to restore chronological order
        results.sort(key=lambda x: x['index'])
        
        logger.info(f"‚úÖ Parallel transcription complete: {len(results)} segments processed and sorted")
        
        # Verify timeline integrity
        if not self._verify_timeline_integrity(results):
            logger.warning("‚ö†Ô∏è Timeline integrity check failed - results may be out of order!")
        
        # Remove index field from results (no longer needed)
        output_segments = []
        for r in results:
            segment = {k: v for k, v in r.items() if k != 'index'}
            output_segments.append(segment)
        
        return output_segments
    
    def _transcribe_segments_sequential(self, processed_segments: List[Tuple]) -> List[Dict]:
        """
        üöÄ TRUE BATCH TRANSCRIPTION: Process segments with encoder batching!
        
        This method now uses batch encoding (parallel) + sequential decoding.
        Much faster than old sequential, same VRAM as single model!
        
        Args:
            processed_segments: List of (segment_file, original_start, original_end, ...)
            
        Returns:
            List of transcribed segments in chronological order
        """
        batch_size = self.config.get('batch_size', 3)  # Default: 3 segments at once
        total_segments = len(processed_segments)
        
        logger.info(f"üöÄ TRUE BATCH TRANSCRIPTION: {total_segments} segments with batch_size={batch_size}")
        logger.info(f"   1 Model ({self.model_name}), GPU encoder batching, ~12GB VRAM")
        
        all_transcribed_segments = []
        
        # Process in batches
        num_batches = (total_segments + batch_size - 1) // batch_size
        
        with tqdm(total=total_segments, desc="üé§ Batch transcribing", unit="segment") as pbar:
            for batch_idx in range(num_batches):
                start_idx = batch_idx * batch_size
                end_idx = min(start_idx + batch_size, total_segments)
                batch_segments = processed_segments[start_idx:end_idx]
                
                logger.debug(f"üì¶ Batch {batch_idx+1}/{num_batches}: segments {start_idx}-{end_idx-1}")
                
                try:
                    # Extract segment files for encoding
                    segment_files = [seg_info[0] for seg_info in batch_segments]
                    
                    # üöÄ BATCH ENCODE (parallel GPU execution!)
                    audio_features_batch = self._batch_encode_segments(segment_files)
                    
                    # üêå SEQUENTIAL DECODE (autoregressive limitation)
                    batch_results = self._batch_decode_features(audio_features_batch, batch_segments)
                    
                    all_transcribed_segments.extend(batch_results)
                    pbar.update(len(batch_segments))
                    
                except Exception as e:
                    logger.error(f"Batch {batch_idx+1} failed: {e}")
                    # Fallback: transcribe individually
                    for segment_info in batch_segments:
                        segment_file, original_start, original_end = segment_info[:3]
                        try:
                            result = self.transcribe_with_enhanced_options(segment_file)
                            validated = self.validate_transcription_segment(
                                result, (segment_file, original_start, original_end)
                            )
                            all_transcribed_segments.append({
                                "start": original_start / 1000.0,
                                "end": original_end / 1000.0,
                                "text": validated["text"],
                                "confidence": validated.get("confidence"),
                                "quality_score": validated.get("quality_score"),
                                "warnings": validated.get("warnings"),
                                "segment_file": segment_file
                            })
                        except Exception as fallback_error:
                            logger.error(f"Fallback failed for {segment_file}: {fallback_error}")
                            all_transcribed_segments.append({
                                "start": original_start / 1000.0,
                                "end": original_end / 1000.0,
                                "text": f"[Error: {fallback_error}]",
                                "segment_file": segment_file,
                                "error": str(fallback_error)
                            })
                        pbar.update(1)
        
        logger.info(f"‚úÖ Batch transcription complete: {len(all_transcribed_segments)} segments")
        return all_transcribed_segments
    
    def _transcribe_whole_file(self, audio_file: str, analysis_data: Dict, start_time: float) -> Dict[str, Any]:
        """
        Transcribe entire audio file without segmentation (bypass splitting).
        Whisper will handle the audio as one piece and return its own segments.
        
        Args:
            audio_file: Path to audio file
            analysis_data: Speech pattern analysis results
            start_time: Start time for tracking processing duration
            
        Returns:
            Complete transcription results in the same format as segmented version
        """
        logger.info("üöÄ Processing entire file through Whisper (no pre-segmentation)")
        
        # Gather transcription configuration
        transcription_config_params = {
            "model_name": self.model_name,
            "device": self.device,
            "language": self.language,
            "segmentation_mode": "none",  # Indicate no pre-segmentation
            "parameters": {
                "whole_file_processing": True,
                "note": "Audio processed as single unit - Whisper internal segmentation only"
            }
        }
        
        try:
            # Transcribe the entire file
            logger.info(f"Starting Whisper transcription for: {audio_file}")
            transcription_result = self.transcribe_with_enhanced_options(audio_file)
            
            # Extract segments from Whisper's result
            whisper_segments = transcription_result.get("segments", [])
            logger.info(f"Whisper returned {len(whisper_segments)} internal segments")
            
            # Convert Whisper segments to our format
            all_transcribed_segments = []
            full_text = transcription_result.get("text", "").strip()
            
            for seg in whisper_segments:
                output_segment = {
                    "start": seg.get("start", 0),
                    "end": seg.get("end", 0),
                    "text": seg.get("text", "").strip(),
                    "confidence": seg.get("avg_logprob", 0.0),  # Whisper uses avg_logprob
                    "quality_score": 1.0,  # Default for whole-file processing
                    "warnings": []
                }
                all_transcribed_segments.append(output_segment)
            
            # Create the nested 'transcription' dictionary
            transcription_details = {
                "text": full_text,
                "segments": all_transcribed_segments,
                "language": self.language,
                "processing_time_seconds": time.time() - start_time
            }
            
            # Consolidate all data
            final_results = {
                "transcription": transcription_details,
                "speech_pattern_analysis": analysis_data,
                "transcription_config": transcription_config_params,
                "audio_file_path": audio_file
            }
            
            logger.info(f"‚úÖ Whole-file transcription completed in {time.time() - start_time:.2f}s")
            return final_results
            
        except Exception as e:
            logger.error(f"‚ùå Error during whole-file transcription: {e}")
            # Return error result
            return {
                "transcription": {
                    "text": "",
                    "segments": [],
                    "language": self.language,
                    "processing_time_seconds": time.time() - start_time,
                    "error": str(e)
                },
                "speech_pattern_analysis": analysis_data,
                "transcription_config": transcription_config_params,
                "audio_file_path": audio_file,
                "warnings": [f"Whole-file transcription failed: {e}"]
            }
    
    def transcribe_audio_file_enhanced(self, audio_file: str) -> Dict[str, Any]:
        """
        Enhanced transcription workflow optimized for slow speakers.
        
        Args:
            audio_file: Path to audio file
            
        Returns:
            Complete enhanced transcription results, including configuration parameters.
        """
        start_time_transcription = time.time() # Renamed to avoid conflict
        
        logger.info(f"Starting enhanced transcription for: {audio_file}")
        
        # Load model
        self.load_model()
        
        # Step 1: Analyze speech patterns
        analysis_data = self.analyze_speech_patterns(audio_file) # Renamed to avoid conflict with the final output dict key
        
        # NEW: Check if segmentation should be bypassed
        if self.config.get('disable_segmentation', False):
            logger.info("‚ö° Segmentation DISABLED - Processing entire file through Whisper")
            return self._transcribe_whole_file(audio_file, analysis_data, start_time_transcription)
        
        # Step 2: Choose segmentation method based on configuration
        segmentation_mode = self.config.get('segmentation_mode', 'defensive_silence')
        audio = AudioSegment.from_file(audio_file)
        
        logger.info(f"Using segmentation mode: {segmentation_mode}")
        
        nonsilent_ranges: List[Tuple[int, int]] = []
        if segmentation_mode == 'fixed_time':
            # Use SegmentationHandler for fixed_time_segmentation
            nonsilent_ranges = self.segmentation_handler.fixed_time_segmentation(audio)
            logger.info(f"Fixed-time segmentation: {len(nonsilent_ranges)} segments")
        elif segmentation_mode == 'defensive_silence':
            nonsilent_ranges = self.defensive_silence_detection(audio, analysis_data)
            logger.info(f"Defensive silence detection: {len(nonsilent_ranges)} segments")
        elif segmentation_mode == 'precision_waveform':
            nonsilent_ranges = self.precision_waveform_detection(audio, analysis_data)
            logger.info(f"Precision waveform detection: {len(nonsilent_ranges)} segments")
        else:
            logger.warning(f"Unknown segmentation mode: {segmentation_mode}. Defaulting to defensive_silence.")
            self.config['segmentation_mode'] = 'defensive_silence' # Correct the config
            segmentation_mode = 'defensive_silence' # Correct the local variable
            nonsilent_ranges = self.defensive_silence_detection(audio, analysis_data)
            logger.info(f"Defensive silence detection (fallback): {len(nonsilent_ranges)} segments")

        # Gather transcription configuration parameters
        transcription_config_params = {
            "model_name": self.model_name,
            "device": self.device,
            "language": self.language, # This is the mapped language code e.g. "de"
            "segmentation_mode": segmentation_mode,
            "parameters": {}
        }
        if segmentation_mode == 'defensive_silence':
            transcription_config_params["parameters"] = {
                "min_silence_len_ms": self.config.get('min_silence_len'),
                "padding_ms": self.config.get('padding'),
                "silence_thresh_offset_db": self.config.get('silence_adjustment') 
            }
        elif segmentation_mode == 'precision_waveform':
            # Parameters are taken from WaveformAnalyzer's config
            transcription_config_params["parameters"] = {
                "frame_duration_ms": self.waveform_analyzer.config.get('frame_duration_ms'),
                "frame_padding_ms": self.waveform_analyzer.config.get('frame_padding_ms'),
                "vad_threshold_energy": self.waveform_analyzer.config.get('vad_threshold_energy'),
                "min_speech_duration_ms": self.waveform_analyzer.config.get('min_speech_duration_ms'),
                "min_silence_duration_ms": self.waveform_analyzer.config.get('min_silence_duration_ms')
            }
        elif segmentation_mode == 'fixed_time':
            transcription_config_params["parameters"] = {
                "fixed_time_duration_ms": self.config.get('fixed_time_duration'),
                "fixed_time_overlap_ms": self.config.get('fixed_time_overlap')
            }

        if not nonsilent_ranges:
            logger.warning("No non-silent ranges detected. Transcription might be empty or incomplete.")
            if len(audio) <= self.config.get('max_segment_length', 240000) and len(audio) > 0:
                 logger.info("Creating a single segment for the entire audio as no specific ranges were found.")
                 nonsilent_ranges = [(0, len(audio))]
            else:
                return {
                    "text": "", 
                    "segments": [], 
                    "language": self.language,
                    "processing_time_seconds": time.time() - start_time_transcription,
                    "speech_pattern_analysis": analysis_data, # Changed key name
                    "transcription_config": transcription_config_params,
                    "warnings": ["No speech detected or audio too long for single segment processing without VAD."]
                }

        # Step 3: Create segments (with or without overlap based on mode)
        # The audio object for segment creation is 'audio' (AudioSegment.from_file(audio_file))
        # The audio file path string for naming is 'audio_file'
        
        logger.info(f"")
        logger.info(f"üìù Detected {len(nonsilent_ranges)} speech segments")
        logger.info(f"üìÅ Now exporting segment audio files to disk...")
        logger.info(f"")
        
        processed_segments: List[Tuple[str, int, int, int, int]] = []
        if segmentation_mode in ['defensive_silence', 'precision_waveform']:
            # These VAD methods produce non-overlapping speech chunks.
            # We use create_non_overlapping_segments to add padding without re-introducing overlaps.
            processed_segments = self.segmentation_handler.create_non_overlapping_segments(
                audio_segment_obj=audio, 
                audio_file_path_str=audio_file, 
                nonsilent_ranges=nonsilent_ranges, 
                analysis=analysis_data
            )
        elif segmentation_mode == 'fixed_time':
            # fixed_time_segmentation already creates segments with desired overlap.
            # We need to adapt its output or how it's used here.
            # The current fixed_time_segmentation returns (start, end) tuples.
            # We need to convert these into the (segment_file, original_start, original_end, padded_start, padded_end) format.
            # For fixed_time, original_start/end are the same as padded_start/end as overlap is internal.
            
            temp_dir = Path(audio_file).parent / "temp_segments"
            temp_dir.mkdir(exist_ok=True)
            segment_prefix = self.config.get('segment_prefix', 'segment')

            for i, (start_ms, end_ms) in enumerate(nonsilent_ranges):
                segment_audio = audio[start_ms:end_ms]
                # Use a consistent naming scheme, perhaps including the original audio file's stem
                segment_file_name = f"{Path(audio_file).stem}_{segment_prefix}_{i:03d}_{start_ms}-{end_ms}.wav"
                segment_file_path = temp_dir / segment_file_name
                
                try:
                    segment_audio.export(str(segment_file_path), format="wav")
                    # For fixed_time, original and padded times are the same as segment boundaries
                    processed_segments.append((str(segment_file_path), start_ms, end_ms, start_ms, end_ms))
                except Exception as e:
                    logger.error(f"Failed to export fixed-time segment {segment_file_path}: {e}")
            logger.info(f"Created {len(processed_segments)} segment files for fixed_time mode.")
        else:
            # This case should not be reached due to earlier fallback, but as a safeguard:
            logger.error(f"Internal error: Unhandled segmentation mode '{segmentation_mode}' at segment creation stage.")
            return {
                "text": "", "segments": [], "language": self.language, 
                "processing_time_seconds": time.time() - start_time_transcription, 
                "speech_pattern_analysis": analysis_data, # Changed key name
                "transcription_config": transcription_config_params,
                "warnings": [f"Internal error: Unhandled segmentation_mode {segmentation_mode}"]
            }

        if not processed_segments:
            logger.warning("No segments were created for transcription.")
            # Clean up temp_dir if it was created and is empty
            if segmentation_mode == 'fixed_time' and 'temp_dir' in locals() and temp_dir.exists():
                try:
                    if not any(temp_dir.iterdir()): # Check if directory is empty
                        temp_dir.rmdir()
                except OSError as e:
                    logger.warning(f"Could not remove empty temp_segment directory {temp_dir}: {e}")

            return {
                "text": "", "segments": [], "language": self.language, 
                "processing_time_seconds": time.time() - start_time_transcription, 
                "speech_pattern_analysis": analysis_data, # Changed key name
                "transcription_config": transcription_config_params,
                "warnings": ["No segments created for transcription."]
            }
            
        # Step 4: Transcribe segments (parallel or sequential)
        max_workers = self.config.get('parallel_workers', 1)
        
        if max_workers > 1:
            # Use parallel processing
            logger.info(f"")
            logger.info(f"üöÄ Starting PARALLEL transcription with {max_workers} workers")
            logger.info(f"   Each worker will load its own model (~{max_workers * 3}GB VRAM)")
            logger.info(f"   Monitor GPU utilization - should be much higher than sequential mode")
            logger.info(f"")
            all_transcribed_segments = self._transcribe_segments_parallel(processed_segments, max_workers)
        else:
            # Use sequential processing (default)
            all_transcribed_segments = self._transcribe_segments_sequential(processed_segments)
        
        # Build full text from all segments
        full_text = " ".join(seg.get('text', '') for seg in all_transcribed_segments)
        
        # Step 5: Clean up segment files (after all workers complete!)
        if self.config.get('cleanup_segments', True):
            logger.info("üßπ Cleaning up segment files...")
            for segment in all_transcribed_segments:
                segment_file = segment.get('segment_file')
                if segment_file:
                    try:
                        Path(segment_file).unlink(missing_ok=True)
                    except FileNotFoundError:
                        pass
                    except Exception as e:
                        logger.warning(f"Could not delete segment file {segment_file}: {e}")
        
        # Clean up temp_dir if it was created for fixed_time mode
        if segmentation_mode == 'fixed_time' and 'temp_dir' in locals() and temp_dir.exists():
            try:
                # Delete remaining files in temp_dir first
                for item in temp_dir.iterdir():
                    item.unlink()
                temp_dir.rmdir()
                logger.info(f"Cleaned up temporary segment directory: {temp_dir}")
            except Exception as e:
                logger.warning(f"Could not fully clean up temp_segment directory {temp_dir}: {e}")
        
        # Step 5: Consolidate results
        # Create the nested 'transcription' dictionary
        transcription_details = {
            "text": full_text.strip(),
            "segments": all_transcribed_segments,
            "language": self.language, # Assuming self.language is set appropriately in the class
            "processing_time_seconds": time.time() - start_time_transcription
        }

        # Consolidate all data into the final results dictionary
        final_results = {
            "transcription": transcription_details, # Nested transcription data
            "speech_pattern_analysis": analysis_data,
            "transcription_config": transcription_config_params,
            "audio_file_path": audio_file # audio_file is the input path to this method
        }
        
        # self.logger.debug(f"Returning enhanced transcription results with new structure: {{key_count: {len(final_results)}}}")
        return final_results

# Example usage (for testing purposes)